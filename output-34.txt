Createa a new MVC project (with the default template), add a breakpoint somewhere in HomeController.Index and add a watch on "ViewData" - Expand the "Raw View" and "base" ad infinitum. This isn't an MVC issue, just the quickest example I could come up. It might not work, but using this approach you'd remove the event and the timer (removing code is always good!). Instead you'd rely on the timeout functionality of the Join method to cancel the thread. From what you showed, your event+timer is nothing more than a timeout mechanism. The people who voted this up did so largely because they have a sense of humor. Of course, in Ruby, the same would be much more widely accepted - largely because language support (vs framework support seen here) for such meta programming. Of course, this wasn't a Ruby question, but even if you don't see value in learning how and why this is ok in a different world, it at least suggests Reflection (which none of the answers did), which is a good thing to be familiar with (if for no other reason than to know that this isn't a good use for it). thanks for the great answer, it really helped me. I'm not sure if it changed, but it seems it's "url_prefix" and not just "prefix". I think it's the right approach as well. Note that using an update with $set rather than a save might be more efficient: db.players.update({_id: p._id}, {$set: {rank: rank}}) or something Also note that eval locks the entire database. This won't scale, either with a lot of concurrent request and/or as players grows. FWIW, I was doing ranking in MongoDB also, and I switched that specific logic to a Redis sorted set. I blogged about my positive experience: http://openmymind.net/2011/5/8/Practical-NoSQL-Solving-a-Real-Problem-w-Mongo-Red/ That's true, but it also means you should probably refactor your code. I don't think I've ever needed to declare a variable outside of it's intended scope. (I'm not positive about this, but...)AS3 uses a non-deterministic garbage collection. Which means that unreferenced memory will be freed up whenever the runtime feels like it (typically not unless there's a reason to run, since it's an expensive operation to execute). This is the same approach used by most modern garbage collected languages (like C# and Java as well).Assuming there are no other references to the memory pointed to by byteArray or the items within the array itself, the memory will be freed at some point after you exit the scope where byteArray is declared.You can force a garbage collection, though you really shouldn't. If you do, do it only for testing... if you do it in production, you'll hurt performance much more than help it.To force a GC, try (yes, twice):flash.system.System.gc();flash.system.System.gc();You can read more here. The best explanation I've seen is in Chapter 7 of the free Foundations of Programming e-book.Basically, in .NET a memory leak occurs when referenced objects are rooted and thus cannot be garbage collected. This occurs accidentally when you hold on to references beyond the intended scope.You'll know that you have leaks when you start getting OutOfMemoryExceptions or your memory usage goes up beyond what you'd expect (PerfMon has nice memory counters). Understanding .NET's memory model is your best way of avoiding it. Specifically, understanding how the garbage collector works and how references work â€” again, I refer you to chapter 7 of the e-book. Also, be mindful of common pitfalls, probably the most common being events. If object A is registered to an event on object B, then object A will stick around until object B disappears because B holds a reference to A. The solution is to unregister your events when you're done. Of course, a good memory profile will let you see your object graphs and explore the nesting/referencing of your objects to see where references are coming from and what root object is responsible (red-gate ants profile, JetBrains dotMemory, memprofiler are really good choices, or you can use the text-only WinDbg and SOS, but I'd strongly recommend a commercial/visual product unless you're a real guru).I believe unmanaged code is subject to its typical memory leaks, except that shared references are managed by the garbage collector. I could be wrong about this last point. The functionality you're looking for is traditionally called an UPSERT. Atleast knowing what it's called might help you find what you're looking for.I don't think SQL Server 2005 has any great ways of doing this. 2008 introduces the MERGE statement that can be used to accomplish this as shown in: http://www.databasejournal.com/features/mssql/article.php/3739131 or http://blogs.conchango.com/davidportas/archive/2007/11/14/SQL-Server-2008-MERGE.aspxMerge was available in the beta of 2005, but they removed it out in the final release. Any assemblies that are expected to be in the GAC should stay in the GAC. This includes System.web.dll or any other 3rd party dll that you'll deploy to the GAC in production. This means a new developer would have to install these assemblies.All other 3rd party assemblies should be references through a relative path. My typical structure is:-Project--Project.sln--References---StructureMap.dll---NUnit.dll---System.Web.Mvc.dll--Project.Web---Project.Web.Proj---Project.Web.Proj files--Project---Project.Proj---Project.Proj filesProject.Web and Project reference the assemblies in the root/References folder relatively. These .dlls are checked into subversion.Aside from that, */bin */bin/* obj should be in your global ignore path.With this setup, all references to assemblies are either through the GAC (so should work across all computers), or relative to each project within your solution. I've taken to hand-coding all of my DDL (creates/alter/delete) statements, adding them to my .sln as text files, and using normal versioning (using subversion, but any revision control should work). This way, I not only get the benefit of versioning, but updating live from dev/stage is the same process for code and database - tags, branches and so on work all the same.Otherwise, I agree redgate is expensive if you don't have a company buying it for you. If you can get a company to buy it for you though, it really is worth it! I'm with Peter. Developer don't seem to understand passwords. We all pick (and I'm guilty of this too) MD5 or SHA1 because they are fast. Thinking about it ('cuz someone recently pointed it out to me) that doesn't make any sense. We should be picking a hashing algorithm that's stupid slow. I mean, on the scale of things, a busy site will hash passwords what? every 1/2 minute? Who cares if it take 0.8 seconds vs 0.03 seconds server wise? But that extra slowness is huge to prevent all types of common brute-forcish attacks.From my reading, bcrypt is specifically designed for secure password hashing. It's based on blowfish, and there are many implementation.For PHP, check out PHPPass http://www.openwall.com/phpass/For anyone doing .NET, check out BCrypt.NET http://derekslager.com/blog/posts/2007/10/bcrypt-dotnet-strong-password-hashing-for-dotnet-and-mono.ashx I think NUnit is your best bet. With TestDriven.NET, you get great integration within VS.NET. (Resharper also has a unit test runner if you're using it). NUnit it simple to use and follows an established paradigm. You'll also find plenty of projects/tutorials/guides using it which always helps.Your other main choice is probably MBUnit, which is more and more position itself as the BDD framework of choice (in conjunction with Gallio http://www.gallio.org). The Activator class within the root System namespace is pretty powerful.There are a lot of overloads for passing parameters to the constructor and such. Check out the documentation at:  http://msdn.microsoft.com/en-us/library/system.activator.createinstance.aspxor (new path) https://docs.microsoft.com/en-us/dotnet/api/system.activator.createinstanceHere are some simple examples:ObjectType instance = (ObjectType)Activator.CreateInstance(objectType);ObjectType instance = (ObjectType)Activator.CreateInstance("MyAssembly","MyNamespace.ObjectType"); You're looking for SandcastleProject Page: Sandcastle ReleasesBlog: Sandcastle BlogNDoc Code Documentation Generator for .NET used to be the tool of choice, but support has all but stopped. Well, it's distributed. Benchmarks indicate that it's considerably faster (given its distributed nature, operations like diffs and logs are all local so of course it's blazingly faster in this case), and working folders are smaller (which still blows my mind).When you're working on subversion, or any other client/server revision control system, you essentially create working copies on your machine by checking-out revisions. This represents a snapshot in time of what the repository looks like. You update your working copy via updates, and you update the repository via commits.With a distributed version control, you don't have a snapshot, but rather the entire codebase. Wanna do a diff with a 3 month old version? No problem, the 3 month old version is still on your computer. This doesn't only mean things are way faster, but if you're disconnected from your central server, you can still do many of the operations you're used to. In other words, you don't just have a snapshot of a given revision, but the entire codebase.You'd think that Git would take up a bunch of space on your harddrive, but from a couple benchmarks I've seen, it actually takes less. Don't ask me how. I mean, it was built by Linus, he knows a thing or two about filesystems I guess. Depends on the version, 4 is by value, 5 is by reference. http://www.php.net/manual/en/migration5.oop.php In PHP 5 there is a new Object Model. PHP's handling of objects has been completely rewritten, allowing for better performance and more features. In previous versions of PHP, objects were handled like primitive types (for instance integers and strings). The drawback of this method was that semantically the whole object was copied when a variable was assigned, or passed as a parameter to a method. In the new approach, objects are referenced by handle, and not by value (one can think of a handle as an object's identifier). Given a relatively simple CSS:  div { width: 150px; } &lt;div&gt; 12333-2333-233-23339392-332332323 &lt;/div&gt;   How do I make it so that the string stays constrained to the width of 150, and simply wraps to a newline on the hyphen? Well, you didn't specify Rails, so I'm going to throw Shoes out there. First, building shoes apps is probably the best way to learn Ruby (Rails is great, but I find mastering Ruby far more fun/useful). Secondly, while I certainly don't think building crossplatform UI components is trivial, shoes is relatively new, and relatively small. There are no doubt countless additions that could be made. \[start\](.*?)\[end\]Zhich'll put the text in the middle within a capture. I don't disagree with Dan (although a better choice may just be not to answer)...but...Unit testing is the process of writing code to test the behavior and functionality of your system.Obviously tests improve the quality of your code, but that's just a superficial benefit of unit testing. The real benefits are to:Make it easier to change the technical implementation while making sure you don't change the behavior (refactoring). Properly unit tested code can be aggressively refactored/cleaned up with little chance of breaking anything without noticing it.Give developers confidence when adding behavior or making fixes.Document your codeIndicate areas of your code that are tightly coupled. It's hard to unit test code that's tightly coupledProvide a means to use your API and look for difficulties early onIndicates methods and classes that aren't very cohesiveYou should unit test because its in your interest to deliver a maintainable and quality product to your client.I'd suggest you use it for any system, or part of a system, which models real-world behavior. In other words, it's particularly well suited for enterprise development. I would not use it for throw-away/utility programs. I would not use it for parts of a system that are problematic to test (UI is a common example, but that isn't always the case)The greatest pitfall is that developers test too large a unit, or they consider a method a unit. This is particularly true if you don't understand Inversion of Control - in which case your unit tests will always turn into end-to-end integration testing. Unit test should test individual behaviors - and most methods have many behaviors.The greatest misconception is that programmers shouldn't test. Only bad or lazy programmers believe that. Should the guy building your roof not test it? Should the doctor replacing a heart valve not test the new valve? Only a programmer can test that his code does what he intended it to do (QA can test edge cases - how code behaves when it's told to do things the programmer didn't intend, and the client can do acceptance test - does the code do what what the client paid for it to do) I think you're best least hackish way is to consider composition as opposed to inheritance.Or, you could create an interface that has the members you want, have your derived class implement that interface, and program against the interface. Sealed classes should provide a performance improvement. Since a sealed class cannot be derived, any virtual members can be turned into non-virtual members.Of course, we're talking really small gains. I wouldn't mark a class as sealed just to get a performance improvement unless profiling revealed it to be a problem. in 1.x there used to be things DataTables couldn't do which DataSets could (don't remember exactly what). All that was changed in 2.x. My guess is that's why a lot of examples still use DataSets. DataTables should be quicker as they are more lightweight. If you're only pulling a single resultset, its your best choice between the two. Use subversion, it's easy to setup, easy to use, and has plenty of tools. Any future revision system will have an import from SVN feature, so it isn't like you can't change down the road if your needs grow. You're better off using prepared statements with placeholders. Are you using PHP, .NET...either way, prepared statements will provide more security, but I could provide a sample. http://www.opensourcetemplates.org/ has nice designs, just not enough selection. No don't null objects. You can check out http://codebetter.com/blogs/karlseguin/archive/2008/04/27/foundations-of-programming-pt-7-back-to-basics-memory.aspx for more information, but setting things to null won't do anything, except dirty your code. This question is loaded, data driven design vs domain driven design. For any application that has a good amount of behavior, then domain driven design should be preferred. Reporting, or utility applications tend to work better (or are quicker to develop) with data driven design.What you're asking is "should my company make a fundamental shift in how we design our code". As a domain-freak, my gut reaction is to scream yes. However, by the simple nature of your question, I'm not sure you fully understand the scope of the change you are proposing. I think you should talk more to your team about it. Get some literature, such as Evan's DDD book, or the free foundations ebook, and then you'll be in a better position to judge which direction you should go. IE 8 is supposed to have better tools, but the IE Developer Toolbar is pretty good. e-texteditor seems to be growing as the editor of choice for rails development on ruby. Too bad it isn't free.Aside from that, the RailsOnWindows guide works fine. And Sqlite is by far your best choice for development: RailsWithSqlite There's a good explanation here for .NET.A lot of people are surprise that reference objects are actually passed by value (in both C# and Java). It's a copy of a stack address. This prevents a method from changing where the object actually points to, but still allows a method to change the values of the object. In C# its possible to pass a reference by reference, which means you can change where an actual object points to. this is what I use for C# w/resharper, should work just the same with vb.net:build deploy */bin */bin/* obj *.dll *.pdb *.user *.suo _ReSharper* *.resharper* bin There are three key components (assuming ur using SQL server):SQLConnectionSqlCommandSqlDataReader(if you're using something else, replace Sql with "Something", like MySqlConnection, OracleCommand)Everything else is just built on top of that.Example 1:using (SqlConnection connection = new SqlConnection("CONNECTION STRING"))using (SqlCommand command = new SqlCommand()){ command.commandText = "SELECT Name FROM Users WHERE Status = @OnlineStatus"; command.Connection = connection; command.Parameters.Add("@OnlineStatus", SqlDbType.Int).Value = 1; //replace with enum connection.Open(); using (SqlDataReader dr = command.ExecuteReader)) { List&lt;string&gt; onlineUsers = new List&lt;string&gt;(); while (dr.Read()) { onlineUsers.Add(dr.GetString(0)); } }}Example 2:using (SqlConnection connection = new SqlConnection("CONNECTION STRING"))using (SqlCommand command = new SqlCommand()){ command.commandText = "DELETE FROM Users where Email = @Email"; command.Connection = connection; command.Parameters.Add("@Email", SqlDbType.VarChar, 100).Value = "user@host.com"; connection.Open(); command.ExecuteNonQuery();} Are there any good grid-hosting companies out there that offer .NET stacks? Something like MediaTemple - which won't host the worlds fastest websites, but for the price is far better than "shared hosting". I've used Rackspace's Mosso, but it sucked - it never felt like a normal .NET stack (caching was odd, site recompilation was odd). The benefits part has recently been covered, as for where to start....on a small enterprisey system where there aren't too many unknowns so the risks are low. If you don't already know a testing framework (like NUnit), start by learning that. Otherwise start by writing your first test :) Not really. You can use reflection to achieve what you want, but it won't be nearly as simple as in Javascript. For example, if you wanted to set the private field of an object to something, you could use this function:protected static void SetField(object o, string fieldName, object value){ FieldInfo field = o.GetType().GetField(fieldName, BindingFlags.Instance | BindingFlags.NonPublic); field.SetValue(o, value);} Sure, download the 3.5 redistributable, install it on the servre, and you're good to go. .NET versions can be installed side-by-side, so it won't disrupt any "legacy" apps.http://www.microsoft.com/downloads/details.aspx?FamilyId=333325FD-AE52-4E35-B531-508D977D32A6&amp;displaylang=en I think the standard, within .NET, is to try to do it when possible, but not to create unnecessarily deep structures just to adhere to it as a hard rule. None of my projects follow the namespace == structure rule 100% of the time, sometimes its just cleaner/better to break out from such rules.In Java you don't have a choice. I'd call that a classic case of what works in theory vs what works in practice. try thisUPDATE skills SET level = level + 1 WHERE id = $id it tends to take an inexperienced team longer to build 3-tier.It's more code, so more bugs. I'm just playing the devil's advocate though. I'm surprised no one got this one yet. There's a refactoring specifically for this type of problem:http://www.refactoring.com/catalog/decomposeConditional.html Given an empty method body, will the JIT optimize out the call (I know the C# compiler won't). How would I go about finding out? What tools should I be using and where should I be looking?Since I'm sure it'll be asked, the reason for the empty method is a preprocessor directive.@Chris:Makes sense, but it could optimize out calls to the method. So the method would still exist, but static calls to it could be removed (or at least inlined...)@Jon:That just tells me the language compiler doesn't do anything. I think what I need to do is run my dll through ngen and look at the assembly. In VS.NET, when you add an item to the watch, why is the base property often (always??) endlessly recursive? You shouldn't save anything for the sake of saving it. you should save it because you need it (i.e., QA uses nightly builds to test). At which point, "how long to save it" becomes however long QA wants them.i wouldn't "save" source code so much as tag/label it. I don't know what source control you're using, but tagging is trivial (performance &amp; disk space) for any quality source control system. Once your build is tagged, unless you need binaries, there really isn't any benefit to just having them around because you can simply re-compile when necessary from source.Most CI tools let you tag on each successful build. This can become problematic for some systems as you can easily have 100+ tags a day. For such cases I recommend still running a nightly build and only tagging that. This might not work for you, but based on the information you've provided, I'd suggest looking at thread.Join(XXX) where XXX is the number of milliseconds to wait. It'll greatly simplify your code.http://msdn.microsoft.com/en-us/library/6b1kkss0.aspxyou can block the calling thread to the new thread for a specified amount of time, after which you can abort the Resolve thread.resolveThread.Start();resolveThread.Join(2000); //this will block the main thread, thus making resolve synchronousresolveThread.Abort(); //timeout has expired so you're looking for the type regular expressions we use for validating?telephone (various international formats), postal code, zip code, credit card #s, email, dates, digits, ssn, urls (http, ftp, ...) Noticed the same thing, I fixed it with the following, though I'm not sure if it's the "right" solution:&lt;% Html.RenderPartial("xxx", new ViewDataDictionary(ViewData.Model.Colors)); %&gt; typeof(Path).GetField("InvalidFileNameChars", BindingFlags.NonPublic | BindingFlags.Static).SetValue(null, new[] { 'o', 'v', 'e', 'r', '9', '0', '0', '0' }); Storing decimals as integers is something that people are doing. If you go through the decimal-support feature request in MongoDB's jira, you'll see a couple mention that that's their workaround:https://jira.mongodb.org/browse/SERVER-1393Note that the factor will compound when you do map reduce and stuff which multiplies/divides two or more such numbers. As for the lack of transactions, it does sound like it won't be a problem for you today, but what about tomorrow? If your API will always deal with small units of work, then sure. 2-stage commits are always possible too.I think it's great to learn new technologies...not going to suggest you don't. But consider that schemaless really shines when you need to query on those fields. If you don't, then you could just store them all in a single blobbed-up field/column in either a document or a relational database. (although, every time I've seen a project do this, they 3 months down the road they need to start querying/reporting/something on one of those serialized values, and that's a real pain).     